{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSXb66VYUvaBCLyCM2lnAa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jatingpt/GenAI-Cold-Email-Generator-Project/blob/main/Gen_AI_Cold_Email_Generator_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Problem Statement: Enhancing Client Acquisition through Targeted Cold Emails in IT Services**\n",
        "\n",
        "**In today’s competitive IT landscape, software services companies like TechNova Solutions, InfoSphere Technologies, and Atliq Systems are key enablers for global enterprises by providing scalable and skilled technical support. These companies maintain large pools of software engineers and offer their expertise to clients such as StrideWear (a sportswear brand), FinCore Capital (a financial services firm), and many others.**\n",
        "\n",
        "**To grow their business, these service companies rely heavily on their sales and business development teams to acquire new projects. One of the most effective techniques used is cold emailing—a strategic outreach method to potential clients who may require technical support but haven’t directly engaged with the service provider yet.**\n",
        "\n",
        "**For example, a Business Development Executive from TechNova Solutions might visit the careers page of StrideWear and notice multiple job openings for roles such as Data Engineers, AI Specialists, or Full Stack Developers. These openings typically indicate an ongoing project or a future initiative that requires specialized skills.**\n",
        "\n",
        "**Instead of letting StrideWear go through a lengthy recruitment process via job portals like LinkedIn or Naukri, TechNova can pitch a faster and cost-effective solution. Through a cold email, they propose allocating their already-trained and project-ready engineers to support or even fully deliver the required project. They can also include case studies or links to similar past projects to demonstrate credibility.**\n",
        "\n",
        "* **This approach benefits both parties:**\n",
        "\n",
        "**TechNova acquires a new project and increases its revenue.**\n",
        "\n",
        "**StrideWear saves time and resources by avoiding the traditional hiring process and gains access to expert talent immediately.**\n",
        "\n",
        "**This strategy helps IT service providers penetrate new markets, strengthen partnerships, and offer clients flexibility without long-term employment commitments.**"
      ],
      "metadata": {
        "id": "9niUeAQtRRO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required Packages\n",
        "!pip install langchain langchain-community langchain-core langchain-groq chromadb pandas\n",
        "\n",
        "# Import Libraries\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "import pandas as pd\n",
        "import uuid\n",
        "import chromadb\n"
      ],
      "metadata": {
        "id": "KYSJW4xVTZ14",
        "outputId": "91ff15ee-b569-48d2-8b41-0a8199bda2d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.51)\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.28)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.13.1)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.22.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi==0.115.9 (from chromadb)\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.34.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.24.1-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.53b0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.16)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.32.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.32.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.32.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.32.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.53b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.53b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.53b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.53b0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.53b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.53b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-util-http==0.53b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.53b0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.32.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading chromadb-1.0.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading groq-0.22.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.32.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.32.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.32.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.53b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.53b0-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.53b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.53b0-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.32.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.53b0-py3-none-any.whl (7.3 kB)\n",
            "Downloading opentelemetry_sdk-1.32.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.24.1-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading uvicorn-0.34.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53800 sha256=738277c759b4293cf6b1c7886bdad64b7d48a2f75687b22d108919594011c151\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, python-dotenv, pyproject_hooks, overrides, opentelemetry-util-http, opentelemetry-proto, mypy-extensions, mmh3, marshmallow, humanfriendly, httpx-sse, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, typing-inspect, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, pydantic-settings, opentelemetry-semantic-conventions, onnxruntime, kubernetes, groq, fastapi, dataclasses-json, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-groq, opentelemetry-instrumentation-fastapi, langchain-community, chromadb\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.31.1\n",
            "    Uninstalling opentelemetry-api-1.31.1:\n",
            "      Successfully uninstalled opentelemetry-api-1.31.1\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.52b1\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.52b1:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.52b1\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.31.1\n",
            "    Uninstalling opentelemetry-sdk-1.31.1:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.31.1\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-1.0.4 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.9 fastapi-0.115.9 groq-0.22.0 httptools-0.6.4 httpx-sse-0.4.0 humanfriendly-10.0 kubernetes-32.0.1 langchain-community-0.3.21 langchain-groq-0.3.2 marshmallow-3.26.1 mmh3-5.1.0 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.21.0 opentelemetry-api-1.32.0 opentelemetry-exporter-otlp-proto-common-1.32.0 opentelemetry-exporter-otlp-proto-grpc-1.32.0 opentelemetry-instrumentation-0.53b0 opentelemetry-instrumentation-asgi-0.53b0 opentelemetry-instrumentation-fastapi-0.53b0 opentelemetry-proto-1.32.0 opentelemetry-sdk-1.32.0 opentelemetry-semantic-conventions-0.53b0 opentelemetry-util-http-0.53b0 overrides-7.7.0 posthog-3.24.1 pydantic-settings-2.8.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.0 starlette-0.45.3 typing-inspect-0.9.0 uvicorn-0.34.1 uvloop-0.21.0 watchfiles-1.0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Initializing the Language Model (LLM)**\n",
        "In this step, I’m setting up the LLM (Large Language Model) from Groq using the llama3-70b-8192 model. This model will help generate smart, human-like responses — for example, extracting job info from text or writing personalized cold email"
      ],
      "metadata": {
        "id": "V2CrSbusZDgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LLM\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    groq_api_key='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',\n",
        "    model_name=\"llama3-70b-8192\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "YwVgskDeScnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Scraping the Job Description from a Webpage**\n",
        "Here, I'm using WebBaseLoader to load and scrape the content from a job posting URL (in this case, Nike's careers page). This helps us extract the full job description text"
      ],
      "metadata": {
        "id": "xqPqQnUpZUVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape Job Description\n",
        "loader = WebBaseLoader(\"https://jobs.nike.com/job/R-33460\")  #for the future reference if the job is expire then use the other job reference link.\n",
        "page_data = loader.load().pop().page_content\n"
      ],
      "metadata": {
        "id": "Ao7imG8qSgy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Creating a Prompt to Extract Job Information**\n",
        "In this step, I’m creating a prompt template that will be sent to the LLM. It tells the model to look at the scraped job text and extract key details like the role, experience required, skills, and a short job description — all formatted as clean JSON."
      ],
      "metadata": {
        "id": "v1R2gsCLZl_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prompt Template for Job Info Extraction\n",
        "prompt_extract = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    ### SCRAPED TEXT FROM WEBSITE:\n",
        "    {page_data}\n",
        "    ### INSTRUCTION:\n",
        "    The scraped text is from the career's page of a website.\n",
        "    Your job is to extract the job postings and return them in JSON format containing the\n",
        "    following keys: `role`, `experience`, `skills` and `description`.\n",
        "    Only return the valid JSON.\n",
        "    ### VALID JSON (NO PREAMBLE):\n",
        "    \"\"\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "9NCpq80OSlfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Extracting Job Information Using LLM**\n",
        "combining the prompt and the LLM to extract structured job details from the scraped webpage content. This step sends the job description to the model and gets a response with useful info like role, skills, and experience in JSON format."
      ],
      "metadata": {
        "id": "jsH02Og2ZvAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Extracting Job Info\n",
        "chain_extract = prompt_extract | llm\n",
        "res = chain_extract.invoke(input={'page_data': page_data})"
      ],
      "metadata": {
        "id": "5cZBXs-tSqB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mQXWSM9IZp5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TwgiTwoKZp2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Parsing the Extracted Job Info**\n",
        "In this step, I'm using a JSON parser to convert the model’s response into a Python dictionary."
      ],
      "metadata": {
        "id": "3WcZ8CeWaBrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Parse Extracted Info\n",
        "json_parser = JsonOutputParser()\n",
        "job = json_parser.parse(res.content)"
      ],
      "metadata": {
        "id": "-cxkcUsiSuRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Loading My Portfolio and Setting Up ChromaDB**\n",
        "First, I’m loading my portfolio CSV file into a DataFrame — this file contains my projects, tech stack, and related links.\n",
        "\n",
        "Then, I’m creating a vector database using ChromaDB. This will help match job requirements with my most relevant projects by storing and retrieving similar content based on text similarity."
      ],
      "metadata": {
        "id": "LzET5ptqaKxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Loading the Portfolio CSV\n",
        "df = pd.read_csv(\"/content/my_portfolio.csv\")  # Must have columns: Techstack, Links\n",
        "\n",
        "#  Create Vector DB Using ChromaDB\n",
        "client = chromadb.PersistentClient(path='vectorstore')\n",
        "collection = client.get_or_create_collection(name=\"portfolio\")"
      ],
      "metadata": {
        "id": "vIy2mymTSybB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Storing Portfolio Projects in ChromaDB**\n",
        "Here, I’m connecting to a persistent ChromaDB database and creating a collection named \"portfolio\".\n",
        "\n",
        "Then, I’m adding my portfolio projects to the database, where each project’s tech stack becomes a searchable document, and its link is stored as metadata.\n",
        "This is done only once, so the database doesn't get duplicate entries every time the code runs."
      ],
      "metadata": {
        "id": "B4jGmdNKaVw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Creating Vector DB Using ChromaDB\n",
        "client = chromadb.PersistentClient(path='vectorstore')\n",
        "collection = client.get_or_create_collection(name=\"portfolio\")\n",
        "\n",
        "#  Adding to ChromaDB\n",
        "if collection.count() == 0:\n",
        "    for _, row in df.iterrows():\n",
        "        collection.add(\n",
        "            documents=[row[\"Techstack\"]],\n",
        "            metadatas={\"links\": row[\"Links\"]},\n",
        "            ids=[str(uuid.uuid4())]\n",
        "        )\n"
      ],
      "metadata": {
        "id": "KHfUsm_CS4GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Querying Top Matching Portfolio Projects**\n",
        "In this section, I'm extracting and sanitizing the skills from the job description to search for the most relevant projects in my portfolio.\n",
        "\n",
        "If no skills are found, I set default keywords like AI, Python, and Machine Learning as a fallback.\n",
        "\n",
        "If skills are present, I ensure they’re in the proper format and non-empty.\n",
        "\n",
        "The final check ensures that the list of skills isn’t empty before proceeding with the search."
      ],
      "metadata": {
        "id": "Ci42EzjMat7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Query Top Matching Portfolios\n",
        "#  Safely select first job if job is a list\n",
        "if isinstance(job, list):\n",
        "    job = job[0]  # pick the first job posting\n",
        "\n",
        "# Safely extract and sanitize skills\n",
        "raw_skills = job.get('skills', [])\n",
        "if not raw_skills:\n",
        "    print(\" No skills found in job description.\")\n",
        "    skills_query = [\"AI\", \"Python\", \"Machine Learning\"]  # Fallback keywords\n",
        "elif isinstance(raw_skills, list):\n",
        "    skills_query = [str(skill) for skill in raw_skills if isinstance(skill, (str, int, float)) and str(skill).strip()]\n",
        "else:\n",
        "    skills_query = [str(raw_skills)] if str(raw_skills).strip() else [\"AI\", \"Python\"]\n",
        "\n",
        "# Final check\n",
        "if not skills_query:\n",
        "    raise ValueError(\" No valid skills extracted. Cannot proceed with empty query.\")\n"
      ],
      "metadata": {
        "id": "6tcg08PIS4wM",
        "outputId": "16895b37-04ad-4377-d07f-43eb9e5c0603",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " No skills found in job description.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Extracting Relevant Portfolio Links**\n",
        "In this part, I’m running a query on the ChromaDB vector database using the skills we extracted earlier. This finds the top 2 matching portfolio projects based on text similarity.\n",
        "\n",
        "After getting the results, I flatten the metadata list to extract the portfolio project links safely. If the link exists in the metadata, I store it in matched_links for future use in the cold email."
      ],
      "metadata": {
        "id": "eBGos3G4a18k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Flattening the metadata list to extract links safely\n",
        "results = collection.query(query_texts=skills_query, n_results=2)\n",
        "\n",
        "metadatas = results.get('metadatas', [])\n",
        "matched_links = [meta['links'] for sublist in metadatas for meta in sublist if isinstance(meta, dict) and 'links' in meta]\n"
      ],
      "metadata": {
        "id": "bXH55WX_TD0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Creating a Cold Email Prompt**\n",
        "In this step, I'm setting up a prompt template that will instruct the LLM to write a cold email. The email will introduce AtliQ, highlight our expertise in AI & software consulting, and offer solutions tailored to the job description from Nike.\n",
        "\n",
        "The email will also include relevant portfolio links that showcase AtliQ’s capabilities in a similar context. This email will be crafted without any introductory text, keeping it direct and professional."
      ],
      "metadata": {
        "id": "OZ7MQrfua-xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Creating a Prompt for Cold Email\n",
        "prompt_email = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    ### JOB DESCRIPTION:\n",
        "    {job_description}\n",
        "\n",
        "    ### INSTRUCTION:\n",
        "    You are Jatin, a business development executive at AtliQ. AtliQ is an AI & Software Consulting company dedicated to facilitating\n",
        "    the seamless integration of business processes through automated tools.\n",
        "    Over our experience, we have empowered numerous enterprises with tailored solutions, fostering scalability,\n",
        "    process optimization, cost reduction, and heightened overall efficiency.\n",
        "    Your job is to write a cold email to the client regarding the job mentioned above describing the capability of AtliQ\n",
        "    in fulfilling their needs.\n",
        "    Also add the most relevant ones from the following links to showcase Atliq's portfolio: {link_list}\n",
        "    Remember you are Jatin, BDE at AtliQ.\n",
        "    Do not provide a preamble.\n",
        "    ### EMAIL (NO PREAMBLE):\n",
        "    \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "uU0Vy5P3TIAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Generating the Cold Email**\n",
        "In this part, I'm using the prompt template created earlier and feeding it into the LLM to generate the cold email. The LLM will use the provided job description and the most relevant portfolio links to write a personalized email from the perspective of Mohan, the Business Development Executive at AtliQ.\n",
        "\n",
        "The result is a well-crafted email ready to be sent to the client."
      ],
      "metadata": {
        "id": "KvVP0M3cbH6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  Generating the Email\n",
        "chain_email = prompt_email | llm\n",
        "email_response = chain_email.invoke({\n",
        "    \"job_description\": str(job),\n",
        "    \"link_list\": matched_links\n",
        "})\n"
      ],
      "metadata": {
        "id": "0WQd505PTL2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Displaying the Final Cold Email**\n",
        "In this step, I'm printing the final cold email generated by the LLM. This allows me to see the content of the email before sending it. The email is printed with a simple header (\"------ FINAL COLD EMAIL ------\") to clearly separate it from other outputs."
      ],
      "metadata": {
        "id": "MANm88HbbPnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  The Final Email after completion of all the tasks\n",
        "print(\"------ FINAL COLD EMAIL ------\\n\")\n",
        "print(email_response.content)"
      ],
      "metadata": {
        "id": "sQ8ke-tCTPSl",
        "outputId": "83fcb55a-0e5e-4f7a-c745-c62afd09597a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ FINAL COLD EMAIL ------\n",
            "\n",
            "Subject: Revolutionize Your Retail Experience with AtliQ's AI-Powered Solutions\n",
            "\n",
            "Dear Hiring Manager,\n",
            "\n",
            "I came across the job posting for a Nike Athlete (Sales Associate) and was impressed by the emphasis on providing exceptional customer experiences. As a Business Development Executive at AtliQ, I believe our AI-driven solutions can help elevate your retail operations and enhance customer engagement.\n",
            "\n",
            "At AtliQ, we specialize in developing tailored solutions that streamline business processes, reduce costs, and boost overall efficiency. Our expertise in AI, machine learning, and automation can help you:\n",
            "\n",
            "* Optimize inventory management and supply chain logistics\n",
            "* Implement personalized customer recommendations and loyalty programs\n",
            "* Enhance in-store experiences with interactive kiosks and AR/VR technologies\n",
            "* Analyze customer behavior and preferences to inform data-driven decisions\n",
            "\n",
            "Our portfolio showcases our capabilities in developing innovative solutions for various industries. I'd like to highlight our expertise in machine learning and Python development, as demonstrated in our projects: https://example.com/ml-python-portfolio and https://example.com/python-portfolio.\n",
            "\n",
            "By partnering with AtliQ, you can leverage our expertise to create a seamless and engaging retail experience that drives sales, increases customer loyalty, and sets your brand apart from the competition.\n",
            "\n",
            "I'd love to discuss how AtliQ can help you achieve your retail goals. Please let me know if you're interested in exploring our solutions further.\n",
            "\n",
            "Best regards,\n",
            "\n",
            "Jatin\n",
            "Business Development Executive\n",
            "AtliQ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Conclusion**"
      ],
      "metadata": {
        "id": "xXQl1GpibeKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Project: Generative AI-Based Cold Email Generator for IT Service Companies**\n",
        "##**Objective:**\n",
        "To develop a **Generative AI-powered cold email generator** that enables IT services companies to automate and personalize outreach to potential clients by analyzing job postings and identifying project needs in real-time.\n",
        "\n",
        "##**Project Overview:**\n",
        "This project aims to build a smart cold-email generator using cutting-edge GenAI technologies, including **LLMs (via Groq API), ChromaDB** (Vector Database), and web scraping tools. The system is designed to simulate the role of a Business Development Executive in IT services companies by automating the process of lead generation and personalized cold outreach.\n",
        "\n",
        "##**How It Works:**\n",
        "1. **Web Scraping Career Pages:**\n",
        "The system navigates to the careers page of target companies (e.g., product-based firms like StrideWear, FinCore, etc.) and scrapes job listings.\n",
        "\n",
        "2. **Keyword & Intent Extraction using LLMs:**\n",
        "Using LLMs (hosted on Groq API), it extracts critical information from job postings, such as:\n",
        "\n",
        "* Role Titles\n",
        "\n",
        "* Required Skills and Experience\n",
        "\n",
        "* Project Descriptions or Domain\n",
        "\n",
        "* Urgency or Hiring Trends\n",
        "\n",
        "3. **Contextual Understanding and Email Generation:**\n",
        "Based on extracted information, the LLM formulates a **personalized cold email**, offering the company a plug-and-play solution:\n",
        "\n",
        "Instead of hiring an individual, they can **contract a pre-vetted team** or expert from the IT services company.\n",
        "\n",
        "The email also highlights **relevant past project links or case studies** from the service provider’s portfolio.\n",
        "\n",
        "4. **Vector Search for Relevancy (ChromaDB):**\n",
        "ChromaDB helps in searching through internal project records to find the most contextually relevant case studies, which are then embedded into the email to build credibility.\n",
        "\n",
        "##**Business Impact:**\n",
        "**For Service Companies:**\n",
        "Saves time for sales teams, scales outreach, and increases the chances of acquiring high-value projects.\n",
        "\n",
        "**For Product Companies:**\n",
        "Reduces hiring efforts and onboarding time by getting access to skilled professionals on-demand.\n",
        "\n",
        "**Tech Stack:**\n",
        "* **Groq API** -  for high-speed LLM inference and prompt execution\n",
        "\n",
        "* **ChromaDB** - for semantic search and project portfolio retrieval\n",
        "\n",
        "* **LangChain** - for chaining logic and query orchestration\n",
        "\n",
        "* **Python + Scrapy** - for web scraping\n",
        "\n",
        "* **FastAPI/Flask** - for serving as an API endpoint"
      ],
      "metadata": {
        "id": "qY-ojbLzbkZr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j0elxPqXW-UO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}